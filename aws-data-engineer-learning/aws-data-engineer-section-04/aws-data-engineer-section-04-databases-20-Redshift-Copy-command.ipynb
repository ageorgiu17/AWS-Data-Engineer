{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Redshift - Copy Command"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O să discutăm acuma despre cum putem să adăugăm date în Redshift într-un mod cât mai eficient. Cel mai des utilizat mod de a aduce date în Redshift este prin a utiliza comanda \"COPY\". Comanda COPY:\n",
    "\n",
    "- este paralelizată și extrem de eficientă\n",
    "\n",
    "- se pot copia date din S3, EMR, DynamoDB sau alte surse externe (S3 are nevoie de un fișier de manifest și un IAM role)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Această comandă se poate folosi pentru a citi date din mai multe surse simultan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dacă se dorește să ducem date din Redshift în afară, comanda pe care o să o utilizăm este ce \"UNLOAD\". Prin acestă metodă putem să ducem datele dintr-un tabel ce se găsește în Redshit într-un bucket de S3. Ca să realizăm aceste metode de copiere de date și de unload la date din Redshift este indicat să folosim un VPC astfel încât datele respective să circule printr-un network securizat, nu prin internetul public. Ca să realizăm asta trebuie să avem pus la punct partea de Endpoints, de Gateway astfel încât să existe comunicare între servicii prin acel VPC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recent au apărut și anumite funcționalități noi, precum:\n",
    "\n",
    "- Auto-copy from S3\n",
    "\n",
    "    Copierea din S3 se poate face automat. Acest feature scanează un bucket de S3, iar de fiecare dată când există date noi în acel bucket o să se copieze automat în Redshift\n",
    "\n",
    "- Amazon Aurora zero-ETL integration\n",
    "\n",
    "    Copiază datele din Amazon Aurora în Redshift (replicare de date în Redshift din Aurora)\n",
    "\n",
    "- Redshift Streaming Ingestion\n",
    "\n",
    "    Verifică constant un stream de date pentru a le face load în Data Wearhouse (strem precum Kineses Data Stream sau MSK - pentru Kafka)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### COPY command"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dorim să folosim această comandă atunci când trebuie să copiem o mulțime de date din exterior în Redshift. Această comandă se folosește doar pentru datele externe care sunt inserate în Redshit. Pentru datele care deja există în acest Data Wearhouse o să utilizăm comenzi precum \"Inser into .. select\" sau \"create table as\" (comenzi de SQL). Anumite feature-uri ale acestei comenzi sunt:\n",
    "\n",
    "- poate decripta datele din S3. Atunci când copiem datele (cu comanda \"COPY\") din S3, iar datele respective sunt encriptate, această decriptare se va face automat, iar procesul de decriptare este unul destul de rapid\n",
    "\n",
    "- suportă compresii de tipul Gzip, Snappy sau bzip2 pentru a crește procesul de copiere și mai multe\n",
    "\n",
    "- automatic compression option. Analizează datele care sunt introduse în Redshift  și oferă cea mai bună metodă de a face o compresie la aceste date\n",
    "\n",
    "- Atunci când avem un tabel cu multe rânduri și puține coloane ar trebui să utilizăm o singură coloană de COPY atunci când introducem aceste date într-un tabel din Redshift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
